defaults:
  - base
  - _self_

experiment:
  name: ar_finetune
  batch_size: 32
  sample_mode: time_batch
  learning_rate: 0.00001  # Lower learning rate for fine-tuning
  warm_start: ./weights/paper/paper_weights.pth  # Pre-trained weights
  use_warm_start_conf: True
  num_gpus: 1
  max_epochs: 100  # Adjust based on convergence
  eval_epochs: 5   # Evaluate every 5 epochs
  ckpt_epochs: 5   # Save checkpoint every 5 epochs
  num_loader_workers: 4
  use_wandb: True
  wandb_project: "se3-diffusion-ar"
  
data:
  md_trajectory_path: Tau5R2R3_backbone.npz  # Update this path to your AR npz file
  split_train_val: 0.9  # Use 90% for training, 10% for validation
  max_squared_res: 64000  # Will be automatically determined from AR structure size
  
  # Filtering settings - these are less relevant for AR finetuning but kept for compatibility
  filtering:
    allowed_oligomer: null  # Not used for AR data
    max_len: null  # Will be determined from AR structure
    min_len: null
    max_helix_percent: null
    max_loop_percent: null
    min_beta_percent: null
    rog_quantile: null
    subset: null 