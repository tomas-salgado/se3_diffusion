defaults:
  - base
  - _self_

experiment:
  name: ar_finetune
  batch_size: 256
  sample_mode: time_batch
  learning_rate: 0.0001  
  warm_start: ./weights  # Pre-trained weights
  use_warm_start_conf: True
  num_gpus: 2
  max_epochs: 500  
  eval_epochs: 10   
  ckpt_epochs: 10  
  num_loader_workers: 10
  use_wandb: True
  wandb_project: "se3-diffusion-ar"
  early_ckpt: True  # Enable early checkpoint evaluation
  early_stop_patience: 10  # Stop if no improvement for 10 evaluations
  optimizer:
    beta1: 0.9  # Match pretrained Adam beta1
    beta2: 0.999  # Match pretrained Adam beta2
    eps: 1e-8  # Standard Adam epsilon
  
data:
  xtc_path: Tau5R2R3_apo.xtc  
  top_path: Tau5R2R3_apo.pdb  
  split_train_val: 0.9
  max_squared_res: 64000
  csv_path: null
  
  # Increased validation set for better monitoring
  samples_per_eval_length: 2
  num_eval_lengths: 3
  
  # Disable PDB filtering settings
  filtering:
    allowed_oligomer: null
    max_len: null
    min_len: null
    max_helix_percent: null
    max_loop_percent: null
    min_beta_percent: null
    rog_quantile: null
    subset: null