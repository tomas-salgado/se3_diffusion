defaults:
  - base
  - _self_

experiment:
  name: ar_finetune
  batch_size: 16  # Smaller batch size for more stable finetuning
  sample_mode: time_batch
  learning_rate: 0.00005  # Increased from 2e-5 to 5e-5 for better finetuning (half of pretrained)
  warm_start: ./weights  # Pre-trained weights
  use_warm_start_conf: True
  num_gpus: 1
  max_epochs: 50  # Increased epochs but with early stopping
  eval_epochs: 5   # More frequent evaluation
  ckpt_epochs: 5   # Save checkpoints more frequently
  num_loader_workers: 4
  use_wandb: True
  wandb_project: "se3-diffusion-ar"
  early_ckpt: True  # Enable early checkpoint evaluation
  early_stop_patience: 10  # Stop if no improvement for 10 evaluations
  optimizer:
    beta1: 0.9  # Match pretrained Adam beta1
    beta2: 0.999  # Match pretrained Adam beta2
    eps: 1e-8  # Standard Adam epsilon
  
data:
  xtc_path: Tau5R2R3_apo.xtc  
  top_path: Tau5R2R3_apo.pdb  
  split_train_val: 0.9
  max_squared_res: 64000
  csv_path: null
  
  # Increased validation set for better monitoring
  samples_per_eval_length: 2
  num_eval_lengths: 3
  
  # Disable PDB filtering settings
  filtering:
    allowed_oligomer: null
    max_len: null
    min_len: null
    max_helix_percent: null
    max_loop_percent: null
    min_beta_percent: null
    rog_quantile: null
    subset: null

# Add noise schedule settings for finetuning
diffuser:
  noise_scale: 0.9  # Slightly reduced noise for finetuning
  min_t: 0.01
  max_t: 1.0 